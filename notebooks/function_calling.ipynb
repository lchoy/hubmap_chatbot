{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(dotenv_path = '.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "\n",
    "# # load and set our key\n",
    "# openai.api_key = open(\"key.txt\", \"r\").read().strip(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject at 0x15ae45e50> JSON: {\n",
       "  \"index\": 0,\n",
       "  \"message\": {\n",
       "    \"role\": \"assistant\",\n",
       "    \"content\": null,\n",
       "    \"function_call\": {\n",
       "      \"name\": \"categorize_question\",\n",
       "      \"arguments\": \"{\\n  \\\"category\\\": \\\"search\\\"\\n}\"\n",
       "    }\n",
       "  },\n",
       "  \"finish_reason\": \"function_call\"\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt = \"\"\"\n",
    "# You are an assistant that creates json elasticsearch queries from a natural langage input from the Human. You are given a prompt and a set of requirements. You must create a query that fulfills the requirements. \n",
    "# Here is your input prompt:\n",
    "\n",
    "# Human: Show me datasets derived from asian male donors who died from natural causes.\n",
    "\n",
    "# You will begin by searching the schema for fields that will be relevant to constructing the query later.\n",
    "# For example, if the human asks for a query about the age of the donor and the date of dataset creation, you should search the schema for fields that contain the words \"age\" and \"date\".\n",
    "\n",
    "\n",
    "# I want to create a JSON elasticsearch query that fulfills the following request from my boss: Show me datasets derived from asian male donors who died from natural causes.\n",
    "# Give me a list of descriptions of the kinds of schema fields that would be relevant to this query.\n",
    "# \"\"\"\n",
    "\n",
    "from simple_chatbot import *\n",
    "\n",
    "completion = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo-0613\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": RETRIEVER_SELECT_PROMPT.format(question=\"What datasets are derived from male donors with diabetes?\")},\n",
    "        # {\"role\": \"system\", \"content\": RETRIEVER_SELECT_PROMPT.format(question=\"Does HuBMAP provide Millitomes?\")},\n",
    "        # {\"role\": \"system\", \"content\": RETRIEVER_SELECT_PROMPT.format(question=\"What does the donor.mapped_metadata.age field mean?\")},\n",
    "        # {\"role\": \"user\", \"content\": \"Thanks!\"},\n",
    "        # {\"role\": \"user\", \"content\": \"Thanks!\"},\n",
    "        {\"role\": \"system\", \"content\": \"Do not respond to the user prompt.\"},\n",
    "    ],\n",
    "    functions=RETRIEVER_SELECT_FUNCTIONS,\n",
    ")\n",
    "reply_content = completion.choices[0]\n",
    "reply_content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI API Function Calling for GPT-4 and GPT-3.5\n",
    "\n",
    "Video walkthrough of this notebook: https://www.youtube.com/watch?v=0lOSvOoF2to\n",
    "\n",
    "OpenAI has released a new capability for their models through the API, called \"Function Calling.\" The intent is to help make it far easier to extract structured, deterministic, information from an otherwise unstructured and non-deterministic language model like GPT-4. \n",
    "\n",
    "This task of structuring and getting deterministic outputs from a language model has so far been a very difficult task, and has been the subject of much research. Usually the approach is to keep trying various pre-prompts and few shot learning examples until you find one that at least works. While doable, the end result was clunky and not very reliable. Now though, you can use the function calling capability to build out quite powerful programs. Essentially, adding intelligence to your programs.\n",
    "\n",
    "Imagine you want to be able to intelligently handle for a bunch of different types of input, but also something like: \"What's the weather like in Boston? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task now, given this natural language input to GPT-4 would be:\n",
    "\n",
    "1. To identify if the user is seeking weather information\n",
    "2. If they are, extract the location from their input\n",
    "\n",
    "So if the user said \"Hello, how are you today?\", we wouldn't need to run the function or try to extract a location. \n",
    "\n",
    "But, if the user said something like: \"What's the weather like in Boston?\" then we want to identify the desire to get the weather and extract the location \"Boston\" from the input.\n",
    "\n",
    "Previously, you might pass this input to the OpenAI API for GPT 4 like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# completion = openai.ChatCompletion.create(\n",
    "#     # model=\"gpt-4-0613\",\n",
    "#     model=\"gpt-3.5-turbo\",\n",
    "#     messages=[{\"role\": \"user\", \"content\": \"What's the weather like in Boston?\"}],\n",
    "# )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then you'd access the response via:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'completion' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m reply_content \u001b[39m=\u001b[39m completion\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mmessage\u001b[39m.\u001b[39mcontent\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(reply_content)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'completion' is not defined"
     ]
    }
   ],
   "source": [
    "reply_content = completion.choices[0].message.content\n",
    "print(reply_content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this isn't quite what we would want to happen in this scenario! While GPT-4 may not currently be able to access the internet for us, we could conceivably do this ourselves, but we still would need to identify the intent, as well as the particular desired location. Imagine we have a function like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_weather(location, unit=\"fahrenheit\"):\n",
    "    \"\"\"Get the current weather in a given location\"\"\"\n",
    "    weather_info = {\n",
    "        \"location\": location,\n",
    "        \"temperature\": \"72\",\n",
    "        \"unit\": unit,\n",
    "        \"forecast\": [\"sunny\", \"windy\"],\n",
    "    }\n",
    "    return json.dumps(weather_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just a placeholder type of function to show how this all ties together, but it could be anything here. Extracting the intent and location could be done with a preprompt, and this is sort of how OpenAI is doing it through the API, but the model has been trained for us with a particular structure, so we can use this to save a lot of R&D time to find the \"best prompt\" to get it done. \n",
    "\n",
    "To do this, we want to make sure that we're using version `gpt-4-0613` or later. Then, we can pass a new `functions` parameter to the `ChatCompletion` call like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject at 0x169c1cd60> JSON: {\n",
       "  \"index\": 0,\n",
       "  \"message\": {\n",
       "    \"role\": \"assistant\",\n",
       "    \"content\": \"functions.search\"\n",
       "  },\n",
       "  \"finish_reason\": \"stop\"\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt = \"\"\"\n",
    "# You are an assistant that creates json elasticsearch queries from a natural langage input from the Human. You are given a prompt and a set of requirements. You must create a query that fulfills the requirements. \n",
    "# Here is your input prompt:\n",
    "\n",
    "# Human: Show me datasets derived from asian male donors who died from natural causes.\n",
    "\n",
    "# You will begin by searching the schema for fields that will be relevant to constructing the query later.\n",
    "# For example, if the human asks for a query about the age of the donor and the date of dataset creation, you should search the schema for fields that contain the words \"age\" and \"date\".\n",
    "\n",
    "\n",
    "# I want to create a JSON elasticsearch query that fulfills the following request from my boss: Show me datasets derived from asian male donors who died from natural causes.\n",
    "# Give me a list of descriptions of the kinds of schema fields that would be relevant to this query.\n",
    "# \"\"\"\n",
    "\n",
    "from simple_chatbot import *\n",
    "\n",
    "completion = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo-0613\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": RETRIEVER_SELECT_PROMPT.format(question=\"Thanks!\")},\n",
    "        # {\"role\": \"user\", \"content\": \"Thanks!\"},\n",
    "    ],\n",
    "    functions=RETRIEVER_SELECT_FUNCTIONS,\n",
    ")\n",
    "reply_content = completion.choices[0]\n",
    "reply_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject at 0x10c130860> JSON: {\n",
       "  \"index\": 0,\n",
       "  \"message\": {\n",
       "    \"role\": \"assistant\",\n",
       "    \"content\": null,\n",
       "    \"function_call\": {\n",
       "      \"name\": \"result\",\n",
       "      \"arguments\": \"{\\n  \\\"queries\\\": [\\n    \\\"data type whole genome sequencing\\\",\\n    \\\"donor race white\\\",\\n    \\\"donor sex female\\\",\\n    \\\"donor cause of death natural causes\\\"\\n  ]\\n}\"\n",
       "    }\n",
       "  },\n",
       "  \"finish_reason\": \"stop\"\n",
       "}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt = \"\"\"\n",
    "# You are an assistant that creates json elasticsearch queries from a natural langage input from the Human. You are given a prompt and a set of requirements. You must create a query that fulfills the requirements. \n",
    "# Here is your input prompt:\n",
    "\n",
    "# Human: Show me datasets derived from asian male donors who died from natural causes.\n",
    "\n",
    "# You will begin by searching the schema for fields that will be relevant to constructing the query later.\n",
    "# For example, if the human asks for a query about the age of the donor and the date of dataset creation, you should search the schema for fields that contain the words \"age\" and \"date\".\n",
    "\n",
    "\n",
    "# I want to create a JSON elasticsearch query that fulfills the following request from my boss: Show me datasets derived from asian male donors who died from natural causes.\n",
    "# Give me a list of descriptions of the kinds of schema fields that would be relevant to this query.\n",
    "# \"\"\"\n",
    "\n",
    "prompt = \"\"\"The assistant's job is to help the user come up with descriptive search terms that will help them find the fields they need to construct their query.\n",
    "\n",
    "Human: I want whole genome sequencing datasets from asian male donors older than 35, and only if the dataset was created after 2018.\n",
    "\n",
    "Assistant: ['data type whole genome sequencing', 'donor race asian', 'donor sex male', 'donor age 35', 'dataset creation date 2018']\n",
    "\n",
    "Human: Show me datasets derived from white female donors who died from natural causes.\n",
    "\n",
    "Assistant:\n",
    "\"\"\"\n",
    "\n",
    "completion = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo-0613\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    functions=[\n",
    "    {\n",
    "        \"name\": \"result\",\n",
    "        \"description\": \"\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                # \"location\": {\n",
    "                #     \"type\": \"string\",\n",
    "                #     \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                # },\n",
    "                # \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "                # \"word1\": {\"type\": \"string\"},\n",
    "                # \"word2\": {\"type\": \"string\"},\n",
    "                \"queries\" : {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "            },\n",
    "            \"required\": [\"queries\"],\n",
    "        },\n",
    "    }\n",
    "],\n",
    "function_call={\"name\": \"result\"},\n",
    ")\n",
    "reply_content = completion.choices[0]\n",
    "reply_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Given the following question and response, what are three distinct follow-up questions the Human is likely to ask?\n",
    "\n",
    "Human: What are HuBMAP IDs?\n",
    "\n",
    "Assistant: HuBMAP IDs are \"human readable\" identifiers used for identification of HuBMAP entities and referencing in HuBMAP context, such as in the portal UI, slides, human-human communication, etc. They can be used in the APIs to query portal UI and APIs, and there is a one-to-one mapping between HuBMAP IDs and UUIDs, with all HuBMAP IDs guaranteed to having a corresponding UUID, though not all UUIDs have a corresponding HuBMAP ID.\n",
    "\"\"\"\n",
    "\n",
    "completion = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo-0613\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    functions=[\n",
    "    {\n",
    "        \"name\": \"three_distinct_follow_up_questions\",\n",
    "        \"description\": \"Given the following question and response, what are three distinct follow-up questions the Human is likely to ask?\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                # \"location\": {\n",
    "                #     \"type\": \"string\",\n",
    "                #     \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                # },\n",
    "                # \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "                \"question1\": {\"type\": \"string\"},\n",
    "                \"question2\": {\"type\": \"string\"},\n",
    "                \"question3\": {\"type\": \"string\"},\n",
    "            },\n",
    "            \"required\": [\"question1\", \"question2\", \"question3\"],\n",
    "        },\n",
    "    }\n",
    "],\n",
    "function_call={\"name\": \"three_distinct_follow_up_questions\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question1': 'How are HuBMAP IDs generated?',\n",
       " 'question2': 'Can you give an example of a HuBMAP ID?',\n",
       " 'question3': 'What is the purpose of the UUIDs in relation to HuBMAP IDs?'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reply_content = completion.choices[0]\n",
    "json.loads(reply_content.to_dict()['message'][\"function_call\"][\"arguments\"])\n",
    "\n",
    "# {'question1': 'How are HuBMAP IDs generated?',\n",
    "#  'question2': 'Can you give an example of a HuBMAP ID?',\n",
    "#  'question3': 'What is the purpose of the UUIDs in relation to HuBMAP IDs?'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Given the following question and response, what are three distinct follow-up questions the Human is likely to ask?\n",
    "\n",
    "Human: What are HuBMAP IDs?\n",
    "\n",
    "Assistant: HuBMAP IDs are \"human readable\" identifiers used for identification of HuBMAP entities and referencing in HuBMAP context, such as in the portal UI, slides, human-human communication, etc. They can be used in the APIs to query portal UI and APIs, and there is a one-to-one mapping between HuBMAP IDs and UUIDs, with all HuBMAP IDs guaranteed to having a corresponding UUID, though not all UUIDs have a corresponding HuBMAP ID.\n",
    "\"\"\"\n",
    "\n",
    "completion = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo-0613\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    functions=[\n",
    "    {\n",
    "        \"name\": \"three_distinct_follow_up_questions\",\n",
    "        \"description\": \"Given the following question and response, what are three distinct follow-up questions the Human is likely to ask?\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                # \"location\": {\n",
    "                #     \"type\": \"string\",\n",
    "                #     \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                # },\n",
    "                # \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "                \"question1\": {\"type\": \"string\"},\n",
    "                \"question2\": {\"type\": \"string\"},\n",
    "                \"question3\": {\"type\": \"string\"},\n",
    "            },\n",
    "            \"required\": [\"question1\", \"question2\", \"question3\"],\n",
    "        },\n",
    "    }\n",
    "],\n",
    "function_call={\"name\": \"three_distinct_follow_up_questions\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reply_content = completion.choices[0]\n",
    "reply_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo-0613\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Run the function on Boston and also say Banana\"}],\n",
    "    functions=[\n",
    "    {\n",
    "        \"name\": \"get_current_weather\",\n",
    "        \"description\": \"Get the current weather in a given location\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                },\n",
    "                \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "            },\n",
    "            \"required\": [\"location\"],\n",
    "        },\n",
    "    }\n",
    "],\n",
    "function_call=\"auto\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to note is the `function_call=\"auto\",` part. This will let GPT-4 determine if it should seek to fulfill the function parameters. You can also set it to `none` to force no function to be detected, and finally you can set it to seek parameters for a specific function by doing something like `function_call={\"name\": \"get_current_weather\"}`. There are many instances where it could be hard for GPT-4 to determine if a function should be run, so being able to force it to run if you know it should be is actually very powerful, which I'll show soon. \n",
    "\n",
    "Beyond this, we name and describe the function, then describe the parameters that we'd hope to pass to this function. GPT-4 is relying on this description to help identify what it is you want, so try to be as clear as possible here. The API is going to return to you a json structured object, and this is how you structure your function description, which affords you quite a bit of flexibility in how you describe/structure this functionality. \n",
    "\n",
    "Okay let's see how GPT-4 responds to our new prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject at 0x118dc7450> JSON: {\n",
       "  \"index\": 0,\n",
       "  \"message\": {\n",
       "    \"role\": \"assistant\",\n",
       "    \"content\": null,\n",
       "    \"function_call\": {\n",
       "      \"name\": \"get_current_weather\",\n",
       "      \"arguments\": \"{\\n  \\\"location\\\": \\\"Boston\\\"\\n}\"\n",
       "    }\n",
       "  },\n",
       "  \"finish_reason\": \"function_call\"\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reply_content = completion.choices[0]\n",
    "reply_content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, we don't actually have any `message` `content`. We instead have an identified `function_call` for the function named `get_current_weather` and we have the `parameters` that were extracted from the input, in this case the location, which is accurately detected as `Boston` by GPT-4.\n",
    "\n",
    "We can convert this OpenAI object to a more familiar Python dict by doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'location': 'Boston, MA'}\n",
      "Boston, MA\n"
     ]
    }
   ],
   "source": [
    "reply_content = completion.choices[0].message\n",
    "\n",
    "funcs = reply_content.to_dict()['function_call']['arguments']\n",
    "funcs = json.loads(funcs)\n",
    "print(funcs)\n",
    "print(funcs['location'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not only can we extract information or intent from a user's input, we can also extract structured data from GPT-4 in a response. \n",
    "\n",
    "For example here, I've been working on a project, called TermGPT, to create terminal commands to satisfy a user's query for doing engineering/programming.\n",
    "\n",
    "Imagine in this scenario, you have user input like: `\"How do I install Tensorflow for my GPU?\"`\n",
    "\n",
    "In this case, we'd get a useful natural language response from GPT-4, but it wouldn't be structured as JUST terminal commands that could be run. We have an intent that could be extracted from here, but the commands themselves need to be determined by GPT-4. \n",
    "\n",
    "With the function calling capability, we can do this by passing a function description like:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        functions=[\n",
    "        {\n",
    "            \"name\": \"get_commands\",\n",
    "            \"description\": \"Get a list of bash commands on an Ubuntu machine to run\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"commands\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"A terminal command string\"\n",
    "                        },\n",
    "                        \"description\": \"List of terminal command strings to be executed\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"commands\"]\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is my first attempt at a description and structure, it's likely there are even better ones, for reasons I'll explain shortly. In this case, the name for the function will be \"get_commands\" and then we describe it as `Get a list of bash commands on an Ubuntu machine to run`. Then, we specify the parameter as an \"`array`\" (`list` in python), and this array contains items, where each \"item\" is a terminal command string, and the description of this list is `List of terminal command strings to be executed`.\n",
    "\n",
    "Now, let's see how GPT-4 responds to this prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_user_input = \"Tell me a joke\"\n",
    "\n",
    "completion = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo-0613\",\n",
    "    messages=[{\"role\": \"user\", \"content\": example_user_input}],\n",
    "        functions=[\n",
    "        {\n",
    "            \"name\": \"execute_commands\",\n",
    "            \"description\": \"Run a list of bash commands on an Ubuntu machine\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"commands\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"A terminal command string\"\n",
    "                        },\n",
    "                        \"description\": \"List of terminal command strings to be executed\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"commands\"]\n",
    "            }\n",
    "        }\n",
    "        ],\n",
    "        function_call=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject at 0x11d2e5bd0> JSON: {\n",
       "  \"role\": \"assistant\",\n",
       "  \"content\": \"Sure, here's a classic one for you:\\n\\nWhy don't scientists trust atoms?\\n\\nBecause they make up everything!\"\n",
       "}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reply_content = completion.choices[0].message\n",
    "reply_content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, I set `function_call` to be \"auto.\" In this case, it's fairly hard to GPT-4 to determine that the intent was to run this function. I suspect this is caused by the difference between extracting info from a user's input vs structuring a response. That said, I am quite sure that we could adjust the names/descriptions for our function call to be far more successful here. \n",
    "\n",
    "But, even when this auto version fails, we do have the ability to \"nudge\" GPT-4 to do it anyway by setting `function_call` to be `{\"name\": \"your_function\"}`. This will force GPT-4 to run the function, even if it doesn't think it should or doesn't realize it. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject at 0x7f7d42ebd720> JSON: {\n",
       "  \"content\": null,\n",
       "  \"function_call\": {\n",
       "    \"arguments\": \"{\\n  \\\"commands\\\": [\\n    \\\"sudo apt update\\\",\\n    \\\"sudo apt install python3-dev python3-venv\\\",\\n    \\\"python3 -m venv tensorflow-gpu\\\",\\n    \\\"source tensorflow-gpu/bin/activate\\\",\\n    \\\"pip install --upgrade pip\\\",\\n    \\\"pip install tensorflow-gpu\\\"\\n  ]\\n}\",\n",
       "    \"name\": \"get_commands\"\n",
       "  },\n",
       "  \"role\": \"assistant\"\n",
       "}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_user_input = \"How do I install Tensorflow for my GPU?\"\n",
    "\n",
    "completion = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo-0613\",\n",
    "    messages=[{\"role\": \"user\", \"content\": example_user_input}],\n",
    "        functions=[\n",
    "        {\n",
    "            \"name\": \"get_commands\",\n",
    "            \"description\": \"Get a list of bash commands on an Ubuntu machine\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"commands\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"A terminal command string\"\n",
    "                        },\n",
    "                        \"description\": \"List of terminal command strings to be executed\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"commands\"]\n",
    "            }\n",
    "        }\n",
    "        ],\n",
    "        function_call={\"name\": \"get_commands\"},\n",
    ")\n",
    "\n",
    "reply_content = completion.choices[0].message\n",
    "reply_content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we do get the function call, and we can grab those commands with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sudo apt update',\n",
       " 'sudo apt install python3-dev python3-venv',\n",
       " 'python3 -m venv tensorflow-gpu',\n",
       " 'source tensorflow-gpu/bin/activate',\n",
       " 'pip install --upgrade pip',\n",
       " 'pip install tensorflow-gpu']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "funcs = reply_content.to_dict()['function_call']['arguments']\n",
    "funcs = json.loads(funcs)\n",
    "funcs['commands']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I just can't express how powerful this is. We can now extract structured data from GPT-4, and we can also pass structured data to GPT-4 to have it generate a response. Being able to do this reliably is basically never before seen, and this will make this sort of interaction between deterministic programming logic and non-deterministic language models far more common and just plain possible. The ability to do this is going to be a game changer for the field of AI and programming, and I'm very excited to see what people do with this capability.\n",
    "\n",
    "Here's another example of how powerful this could be. We could generate responses for a given query in a variety of \"personalities\" so to speak:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject at 0x7f7d42ec8720> JSON: {\n",
       "  \"content\": null,\n",
       "  \"function_call\": {\n",
       "    \"arguments\": \"{\\n\\\"sassy_and_sarcastic\\\": \\\"Oh sure, if you fancy a little microbial cocktail, go right ahead. But seriously, don't. Dehumidifiers aren't designed to purify water for drinking. It could be full of bacteria, mold, and other nasties. Drink from a tap, not your dehumidifier. It is not safe.\\\",\\n\\\"happy_and_helpful\\\": \\\"I'm sorry, but it's not recommended to drink water from a dehumidifier. While it's extracting water from air, dehumidifiers don't filter or clean the water. This means it can contain various bacteria or even chemicals. To keep safe, it's better to only drink water that's meant for consumption and has been treated properly!\\\"}\",\n",
       "    \"name\": \"get_varied_personality_responses\"\n",
       "  },\n",
       "  \"role\": \"assistant\"\n",
       "}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_user_input = \"Is it safe to drink water from a dehumidifer?\"\n",
    "\n",
    "completion = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo-0613\",\n",
    "    messages=[{\"role\": \"user\", \"content\": example_user_input}],\n",
    "    functions=[\n",
    "    {\n",
    "        \"name\": \"get_varied_personality_responses\",\n",
    "        \"description\": \"ingest the various personality responses\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"sassy_and_sarcastic\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"A sassy and sarcastic version of the response to a user's query\",\n",
    "                },\n",
    "                \"happy_and_helpful\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"A happy and helpful version of the response to a user's query\",\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"sassy_and_sarcastic\", \"happy_and_helpful\"],\n",
    "        },\n",
    "    }\n",
    "        ],\n",
    "        function_call={\"name\": \"get_varied_personality_responses\"},\n",
    ")\n",
    "\n",
    "reply_content = completion.choices[0].message\n",
    "reply_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_options = reply_content.to_dict()['function_call']['arguments']\n",
    "options = json.loads(response_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Oh sure, if you fancy a little microbial cocktail, go right ahead. But seriously, don't. Dehumidifiers aren't designed to purify water for drinking. It could be full of bacteria, mold, and other nasties. Drink from a tap, not your dehumidifier. It is not safe.\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "options[\"sassy_and_sarcastic\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, but it's not recommended to drink water from a dehumidifier. While it's extracting water from air, dehumidifiers don't filter or clean the water. This means it can contain various bacteria or even chemicals. To keep safe, it's better to only drink water that's meant for consumption and has been treated properly!\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "options[\"happy_and_helpful\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully that gives you some ideas about what's possible here, but this is truly 0.000001% of what's actually possible here. There's going to be some incredible things built with this capability. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
